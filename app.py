import streamlit as st
import pandas as pd
import os
import glob
import joblib
import random
import string
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
from rdkit.Chem import AllChem
from rdkit.Chem import rdFingerprintGenerator
from rdkit import Chem, DataStructs
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, auc
import shap
from Bio import Entrez
from openai import OpenAI

# è®¾ç½®é¡µé¢æ ‡é¢˜å’Œå›¾æ ‡
st.set_page_config(page_title="2025CADDè¯¾ç¨‹å®è·µ", page_icon="ğŸ”¬")

# æ˜¾ç¤ºæ•°æ®é›†æ¦‚å†µçš„å‡½æ•°
def display_data_summary(data):
    st.subheader("æ•°æ®é›†æ¦‚å†µ")

    # æ˜¾ç¤ºæ•°æ®çš„åŸºæœ¬ä¿¡æ¯å’Œæè¿°æ€§ç»Ÿè®¡
    st.write("æ•°æ®çš„åŸºæœ¬ä¿¡æ¯ï¼š")
    st.write(data.info())

    st.write("æè¿°æ€§ç»Ÿè®¡ï¼š")
    st.write(data.describe())

    # é€‰æ‹©æ•°å€¼å‹åˆ—è¿›è¡Œåˆ†å¸ƒç»˜å›¾
    numeric_columns = data.select_dtypes(include=['number']).columns.tolist()

    # ç»˜åˆ¶æ¯ä¸ªæ•°å€¼å‹ç‰¹å¾çš„ç›´æ–¹å›¾
    st.subheader("æ•°å€¼å‹ç‰¹å¾çš„åˆ†å¸ƒ")
    for col in numeric_columns[:3]:
        st.write(f"{col} çš„åˆ†å¸ƒï¼š")
        fig, ax = plt.subplots()
        sns.histplot(data[col], kde=True, ax=ax)
        ax.set_title(f"{col}")
        st.pyplot(fig)
        

# åˆ›å»ºé¡¹ç›®ç›®å½•å¹¶å‘½å
def create_project_directory():
    project_name = datetime.now().strftime("%Y-%m-%d-%H-%M") + "_" + ''.join(random.choices(string.ascii_lowercase + string.digits, k=6))
    project_dir = os.path.join("./projects", project_name)
    os.makedirs(project_dir, exist_ok=True)
    return project_dir

# åˆå§‹åŒ–Fingerprintç”Ÿæˆå™¨
fpgen = rdFingerprintGenerator.GetMorganGenerator(radius=2, fpSize=2048)
# å®šä¹‰æ–°çš„Fingerprintè½¬æ¢å‡½æ•°
def mol_to_fp(smiles):
    mol = Chem.MolFromSmiles(smiles)
    if mol:
        fp = fpgen.GetFingerprint(mol)
        arr = np.zeros((1,))
        DataStructs.ConvertToNumpyArray(fp, arr)
        return arr
    else:
        st.warning(f"æ— æ³•è§£æSMILES: {smiles}")
        return [None] * 2048  # è¿”å›ä¸€ä¸ªå…¨ä¸ºNoneçš„fingerprintï¼Œè¡¨ç¤ºè§£æå¤±è´¥

# å°†fingerprintæ•°æ®ä¿å­˜ä¸ºcsv
def save_input_data_with_fingerprint(data, project_dir, label_column):
    # æ£€æŸ¥SMILESåˆ—ï¼ˆè€ƒè™‘å¤§å°å†™ä¸ä¸€è‡´ï¼‰
    columns_name = 'smiles' if 'smiles' in data.columns else ('SMILES' if 'SMILES' in data.columns else None)
    if columns_name is None:
        st.write('Cannot find column named "smiles" or "SMILES" in the dataset!')
        return
    # è®¡ç®—fingerprintså¹¶åˆå¹¶åˆ°æ•°æ®é›†ä¸­
    fingerprints = data[columns_name].apply(mol_to_fp)
    # åˆ›å»ºDataFrameå¹¶æ·»åŠ æ ‡ç­¾åˆ—
    fingerprint_df = pd.DataFrame(fingerprints.tolist())
    fingerprint_df['label'] = data[label_column]
    # ä¿å­˜ç»“æœä¸ºCSVæ–‡ä»¶
    output_file = os.path.join(project_dir, "input.csv")
    fingerprint_df.to_csv(output_file, index=False)
    # è¿”å›ä¿å­˜çš„æ–‡ä»¶è·¯å¾„ï¼Œæ–¹ä¾¿åç»­æ“ä½œ
    st.write(f"Fingerprint data saved to {output_file}")
    return output_file

# åˆ é™¤ç¼ºå¤±æ•°æ®å¹¶ç¡®ä¿æ•°æ®æ˜¯æ•°å€¼å‹
def preprocess_data(fp_file):
    data = pd.read_csv(fp_file)
    # åˆ é™¤åŒ…å«ç¼ºå¤±å€¼çš„è¡Œ
    data = data.dropna()
    # ç¡®ä¿æ•°æ®éƒ½æ˜¯æ•°å€¼å‹
    # å¯¹äºéæ•°å€¼å‹åˆ—ï¼Œå¯ä»¥è¿›è¡Œè½¬æ¢ï¼Œä¾‹å¦‚ä½¿ç”¨OneHotEncoderç­‰æ–¹æ³•
    for col in data.select_dtypes(include=['object']).columns:
        data[col] = pd.to_numeric(data[col], errors='coerce')  # è½¬æ¢æˆæ•°å€¼å‹ï¼Œå¦‚æœæ— æ³•è½¬æ¢åˆ™ç½®ä¸ºNaN
    # åˆ é™¤åŒ…å«NaNå€¼çš„è¡Œï¼ˆå†æ¬¡ç¡®ä¿æ²¡æœ‰NaNï¼‰
    data = data.dropna()
    return data

# è®­ç»ƒå¹¶ä¿å­˜æ¨¡å‹
def train_and_save_model(fp_file, project_dir, rf_params):
    # é¢„å¤„ç†æ•°æ®
    data = preprocess_data(fp_file)
    # ç‰¹å¾ä¸æ ‡ç­¾åˆ†ç¦»
    X = data.iloc[:, :-1]
    y = data.iloc[:, -1]
    # æ£€æŸ¥Xå’Œyçš„ç»´åº¦
    st.write("ç‰¹å¾æ•°æ®(X)å½¢çŠ¶ï¼š", X.shape)
    st.write("æ ‡ç­¾æ•°æ®(y)å½¢çŠ¶ï¼š", y.shape)
    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    try:
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
    except ValueError as e:
        st.error(f"train_test_split å‡ºé”™ï¼š{e}")
        return None, None
    # åˆå§‹åŒ–æ¨¡å‹
    model = RandomForestClassifier(n_estimators=rf_params['n_estimators'], max_depth=rf_params['max_depth'], max_features=rf_params['max_features'], random_state=42)
    # è®­ç»ƒæ¨¡å‹
    try:
        model.fit(X_train, y_train)
    except Exception as e:
        st.error(f"æ¨¡å‹è®­ç»ƒå¤±è´¥ï¼š{e}")
        return None, None
    # ä¿å­˜æ¨¡å‹
    model_filename = "model.pkl"
    joblib.dump(model, os.path.join(project_dir, model_filename))
    # è¯„ä¼°æ¨¡å‹
    y_pred = model.predict(X_test)
    acc = accuracy_score(y_test, y_pred)

    # è®¡ç®—AUC
    fpr, tpr, _ = roc_curve(y_test, model.predict_proba(X_test)[:, 1])
    roc_auc = auc(fpr, tpr)
    
    # ç»˜åˆ¶AUCæ›²çº¿
    fig, ax = plt.subplots()
    ax.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
    ax.plot([0, 1], [0, 1], color='gray', linestyle='--')
    ax.set_xlabel('False Positive Rate')
    ax.set_ylabel('True Positive Rate')
    ax.set_title('Receiver Operating Characteristic (ROC) Curve')
    ax.legend(loc='lower right')
    plt.savefig(os.path.join(project_dir, "roc_curve.png"))
    st.image(os.path.join(project_dir, "roc_curve.png"))

    # ä¿å­˜è¯„ä¼°ç»“æœ
    confusion = confusion_matrix(y_test, y_pred)
    # ä¿å­˜æ··æ·†çŸ©é˜µå›¾
    fig, ax = plt.subplots()
    sns.heatmap(confusion, annot=True, fmt="d", cmap="Blues", ax=ax)
    ax.set_xlabel("Predicted labels")
    ax.set_ylabel("True labels")
    ax.set_title("Confusion Matrix")
    plt.savefig(os.path.join(project_dir, "confusion_matrix.png"))
    st.image(os.path.join(project_dir, "confusion_matrix.png"))
    
    # ç‰¹å¾é‡è¦æ€§å›¾
    feature_importances = model.feature_importances_
    fig, ax = plt.subplots()
    sns.barplot(x=list(X.columns), y=feature_importances, ax=ax)
    ax.set_title("Feature Importance")
    plt.savefig(os.path.join(project_dir, "feature_importance.png"))
    st.image(os.path.join(project_dir, "feature_importance.png"))
    
    return model, acc, roc_auc

# æŸ¥çœ‹å·²æœ‰çš„é¡¹ç›®
def display_existing_projects():
    projects = glob.glob('./projects/*')
    if not projects:
        st.write("æ²¡æœ‰æ‰¾åˆ°é¡¹ç›®")
    else:
        project_names = [os.path.basename(project) for project in projects]
        project_name = st.selectbox("é€‰æ‹©ä¸€ä¸ªé¡¹ç›®æŸ¥çœ‹", project_names)
        selected_project_dir = os.path.join("./projects", project_name)
        
        # å±•ç¤ºé¡¹ç›®ä¸­çš„æ–‡ä»¶
        if os.path.exists(os.path.join(selected_project_dir, "input.csv")):
            data = pd.read_csv(os.path.join(selected_project_dir, "input.csv"))
            st.write("æ•°æ®é¢„è§ˆï¼š")
            st.dataframe(data.head())
        
        # å±•ç¤ºè¯„ä¼°å›¾è¡¨
        if os.path.exists(os.path.join(selected_project_dir, "confusion_matrix.png")):
            st.image(os.path.join(selected_project_dir, "confusion_matrix.png"))
        
        if os.path.exists(os.path.join(selected_project_dir, "feature_importance.png")):
            st.image(os.path.join(selected_project_dir, "feature_importance.png"))

# æŸ¥è¯¢PubMed Central (PMC) æ•°æ®åº“
def search_pmc(keyword):
    search_term = keyword  # è¾“å…¥æœç´¢å…³é”®è¯
    handle = Entrez.esearch(db="pmc", term=search_term, retmode="xml", retmax=5)  # é™åˆ¶è¿”å›5ç¯‡æ–‡ç« 
    record = Entrez.read(handle)
    return record["IdList"]

# è·å–æ–‡ç« è¯¦ç»†ä¿¡æ¯
def fetch_article_details(pmcid):
    handle = Entrez.efetch(db="pmc", id=pmcid, retmode="xml")
    record = Entrez.read(handle)
    return record

# è·å–å…¨æ–‡é“¾æ¥
def extract_full_text(article_record):
    # æ ¹æ®å…·ä½“çš„XMLç»“æ„æå–æ–‡æœ¬éƒ¨åˆ†
    full_text = ""
    for article in article_record:
        for body in article.get("body", []):
            full_text += body.get("text", "")
    return full_text



# Streamlit UI
st.title("2025CADDè¯¾ç¨‹å®è·µ")

# å·¦ä¾§è¾¹æ é€‰æ‹©åŠŸèƒ½
sidebar_option = st.sidebar.selectbox(
    "é€‰æ‹©åŠŸèƒ½",
    ["æ•°æ®å±•ç¤º", "æ¨¡å‹è®­ç»ƒ", "æ´»æ€§é¢„æµ‹", "æŸ¥çœ‹å·²æœ‰é¡¹ç›®", "çŸ¥è¯†è·å–"]
)

# åŠŸèƒ½1ï¼šå±•ç¤ºæ•°æ®
if sidebar_option == "æ•°æ®å±•ç¤º":
    # åŠ¨æ€åŠ è½½CSVæ–‡ä»¶
    csv_files = glob.glob("./data/*.csv")
    dataset_choice = st.sidebar.selectbox("é€‰æ‹©æ•°æ®é›†", [os.path.basename(file) for file in csv_files])

    # åŠ è½½é€‰å®šçš„æ•°æ®é›†
    selected_file = csv_files[[os.path.basename(file) for file in csv_files].index(dataset_choice)]
    data = pd.read_csv(selected_file)
    
    # æ˜¾ç¤ºæ•°æ®é›†æ¦‚å†µ
    display_data_summary(data)

# åŠŸèƒ½2ï¼šè®­ç»ƒæ¨¡å‹
elif sidebar_option == "æ¨¡å‹è®­ç»ƒ":
    # åŠ¨æ€åŠ è½½CSVæ–‡ä»¶
    csv_files = glob.glob("./data/*.csv")
    dataset_choice = st.sidebar.selectbox("é€‰æ‹©æ•°æ®é›†", [os.path.basename(file) for file in csv_files])

    # åŠ è½½é€‰å®šçš„æ•°æ®é›†
    selected_file = csv_files[[os.path.basename(file) for file in csv_files].index(dataset_choice)]
    data = pd.read_csv(selected_file)
    
    # åŠ¨æ€è·å–æ•°æ®é›†çš„åˆ—åï¼Œå¹¶è®©ç”¨æˆ·é€‰æ‹©æ ‡ç­¾åˆ—
    label_column = st.sidebar.selectbox("é€‰æ‹©æ ‡ç­¾åˆ—", data.columns.tolist())
    
    # è®¾ç½®RandomForestå‚æ•°
    rf_params = {
        'n_estimators': st.sidebar.slider("éšæœºæ£®æ— n_estimators", 50, 500, 100),
        'max_depth': st.sidebar.slider("éšæœºæ£®æ— max_depth", 1, 30, 3),
        'max_features': st.sidebar.slider("éšæœºæ£®æ— max_features", 0.1, 1.0, 0.2)
    }

    # å¼€å§‹å»ºæ¨¡
    if st.sidebar.button("å¼€å§‹è®­ç»ƒæ¨¡å‹"):
        # åˆ›å»ºé¡¹ç›®ç›®å½•
        project_dir = create_project_directory()

        # è®¡ç®—fingerprintå¹¶ä¿å­˜
        fp_file = save_input_data_with_fingerprint(data, project_dir, label_column)
        
        # è®­ç»ƒæ¨¡å‹å¹¶ä¿å­˜ç»“æœ
        model, acc, roc_auc = train_and_save_model(fp_file, project_dir, rf_params)
        
        # æ˜¾ç¤ºè®­ç»ƒç»“æœ
        st.write(f"è®­ç»ƒå®Œæˆï¼Œæ¨¡å‹å‡†ç¡®ç‡(Accuracy): {acc:.4f}; æ¨¡å‹AUC: {roc_auc:.4f}")
        st.success(f"æ¨¡å‹å·²ä¿å­˜åˆ°ï¼š{os.path.join(project_dir, 'model.pkl')}")

# åŠŸèƒ½3ï¼šè¿›è¡Œé¢„æµ‹
elif sidebar_option == "æ´»æ€§é¢„æµ‹":
    # åˆ—å‡ºå·²è®­ç»ƒçš„é¡¹ç›®
    projects = glob.glob('./projects/*')
    if not projects:
        st.write("æ²¡æœ‰æ‰¾åˆ°å·²è®­ç»ƒçš„é¡¹ç›®")
    else:
        project_names = [os.path.basename(project) for project in projects]
        project_name = st.selectbox("é€‰æ‹©ä¸€ä¸ªé¡¹ç›®è¿›è¡Œé¢„æµ‹", project_names)
        selected_project_dir = os.path.join("./projects", project_name)
        
        # åŠ è½½é€‰æ‹©çš„é¡¹ç›®ä¸­çš„æ¨¡å‹
        model_filename = os.path.join(selected_project_dir, "model.pkl")
        if os.path.exists(model_filename):
            model = joblib.load(model_filename)
            st.write(f"åŠ è½½æ¨¡å‹ï¼š{model_filename}")
            
            # è¾“å…¥SMILESå¹¶è¿›è¡Œé¢„æµ‹
            smiles_input = st.text_input("è¾“å…¥åˆ†å­SMILES")
            if smiles_input:
                fingerprint = mol_to_fp(smiles_input)
                if fingerprint is not None:
                    prediction = model.predict([fingerprint])
                    prob = model.predict_proba([fingerprint])[:, -1]
                    st.write(f"é¢„æµ‹ç»“æœ: {prediction[0]}, æ¦‚ç‡: {prob[0]}")
                    
                    # SHAPè§£é‡Š
                    explainer = shap.TreeExplainer(model)
                    shap_values = explainer.shap_values(fingerprint)
                    fig, ax = plt.subplots()
                    shap.summary_plot(shap_values, features=fingerprint, show=False)
                    st.pyplot(fig)
                else:
                    st.write("æ— æ³•è§£æè¯¥SMILESå­—ç¬¦ä¸²ï¼Œè¯·è¾“å…¥æœ‰æ•ˆçš„SMILESã€‚")
        else:
            st.write("æ²¡æœ‰æ‰¾åˆ°æ¨¡å‹æ–‡ä»¶ï¼Œè¯·ç¡®ä¿è¯¥é¡¹ç›®å·²è®­ç»ƒå¹¶ä¿å­˜æ¨¡å‹ã€‚")

# åŠŸèƒ½4ï¼šæŸ¥çœ‹å·²æœ‰é¡¹ç›®
elif sidebar_option == "æŸ¥çœ‹å·²æœ‰é¡¹ç›®":
    display_existing_projects()

# åŠŸèƒ½5:çŸ¥è¯†è·å–
elif sidebar_option == "çŸ¥è¯†è·å–":
    key = st.text_input("è¯·è¾“å…¥æ‚¨çš„OpenAI Key", "")
    
    # è®¾ç½®Entrezé‚®ç®±
    Entrez.email = "your_email@example.com"
    keyword = '"Clinical Toxicology" and "Chemical"'  # æœç´¢å…³é”®è¯
    pmcid_list = search_pmc(keyword)
    st.write(f"å…³é”®è¯: {keyword}")
    st.write(pmcid_list)

    pmcid = pmcid_list[0]
    article_details = fetch_article_details(pmcid)
    st.write(f'Fecth article : {pmcid}')
    st.write(article_details[0]['body']['sec'])
    #full_text = extract_full_text(article_details)

    if key:
        os.environ["OPENAI_API_KEY"] = key
        client = OpenAI()
        # æ–‡çŒ®å†…å®¹ï¼Œå‡è®¾å·²ç»ä»¥å­—ç¬¦ä¸²å½¢å¼æå–
        document_text = str(article_details[0]['body']['sec'])
        # æé—®æ¨¡å‹ä»¥è·å–åŒ–åˆç‰©çš„æ¯’å‰¯ä½œç”¨ä¿¡æ¯
        query = """è¯·ä»ä»¥ä¸‹æ–‡çŒ®ä¸­æå–ä¸æ¯’å‰¯ä½œç”¨ç›¸å…³çš„åŒ–åˆç‰©åå­—ï¼Œåˆ«åæˆ–å…¶ç»“æ„ç­‰ä¿¡æ¯ï¼š\n""" + document_text.replace('\n', '').replace('\n', '')[:10000]
        st.write(query)

        response = client.responses.create(
            model="gpt-4",
            input=query
        )
        st.write(response.output_text)
